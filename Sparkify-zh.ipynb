{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify 项目 Workspace\n",
    "这个 Workspace 包括一个迷你的子数据集（128MB），是完整数据集（12GB）的一个子集。在将你的项目部署到云上之前，你可以自由使用 Workspace 来创建你的项目或用Spark来探索这个较小数据集。设置 Spark 集群的指南可以在选修 Spark 课程的内容里找到。\n",
    "\n",
    "你可以依照下面的步骤进行项目的数据分析和模型搭建部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "import datetime\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.ml.feature import  VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Sparkify Project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载和清洗数据\n",
    "在这个 Workspace 中，小数据集的名称是 `mini_sparkify_event_data.json`.加载和清洗数据集，检查是否有无效或缺失数据——例如，没有userid或sessionid的数据。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_overflow_data = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.json(stack_overflow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
    "df_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.select(\"sessionId\").dropDuplicates().sort(\"sessionId\").show()\n",
    "df_valid = df_valid.filter(df_valid[\"userId\"] != \"\")\n",
    "df_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.select('userId').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 探索性数据分析\n",
    "当你使用完整数据集时，通过加载小数据集，在 Spark 中完成基础操作来实现探索性数据分析。在这个 Workspace 中，我们已经提供给你一个你可以探索的小数据集。\n",
    "\n",
    "### 定义客户流失\n",
    "\n",
    "在你完成初步分析之后，创建一列 `Churn` 作为模型的标签。我建议你使用 `Cancellation Confirmation` 事件来定义客户流失，该事件在付费或免费客户身上都有发生。作为一个奖励任务，你也可以深入了解 `Downgrade` 事件。\n",
    "\n",
    "### 探索数据\n",
    "你定义好客户流失后，就可以执行一些探索性数据分析，观察留存用户和流失用户的行为。你可以首先把这两类用户的数据聚合到一起，观察固定时间内某个特定动作出现的次数或者播放音乐的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_loss_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "df_loss = df_valid.withColumn(\"Churn\", flag_loss_event(\"page\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss.createOrReplaceTempView(\"df_loss_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+\n",
      "|     auth|Churn|   cnt|\n",
      "+---------+-----+------+\n",
      "|Cancelled|  1.0|    52|\n",
      "|Logged In|  0.0|278102|\n",
      "+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT auth, mean(Churn) as Churn,count(*) as cnt\n",
    "          FROM df_loss_table\n",
    "          GROUP BY auth\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+---------+\n",
      "|gender|               Churn|   cnt|sum_churn|\n",
      "+------+--------------------+------+---------+\n",
      "|     F|1.293845178485942...|154578|       20|\n",
      "|     M|2.589499579206318...|123576|       32|\n",
      "+------+--------------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT gender, mean(Churn) as Churn,count(*) as cnt,sum(Churn) as sum_churn\n",
    "          FROM df_loss_table\n",
    "          GROUP BY gender\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+---------+\n",
      "|level|               Churn|   cnt|sum_churn|\n",
      "+-----+--------------------+------+---------+\n",
      "| free|3.768776583334829E-4| 55721|       21|\n",
      "| paid|1.393678096325635...|222433|       31|\n",
      "+-----+--------------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT level, mean(Churn) as Churn,count(*) as cnt,sum(Churn) as sum_churn\n",
    "          FROM df_loss_table\n",
    "          GROUP BY level\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+---------+\n",
      "|method|               Churn|   cnt|sum_churn|\n",
      "+------+--------------------+------+---------+\n",
      "|   PUT|                 0.0|257818|        0|\n",
      "|   GET|0.002557041699449...| 20336|       52|\n",
      "+------+--------------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT method, mean(Churn) as Churn,count(*) as cnt,sum(Churn) as sum_churn\n",
    "          FROM df_loss_table\n",
    "          GROUP BY method\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+---------+\n",
      "|status|               Churn|   cnt|sum_churn|\n",
      "+------+--------------------+------+---------+\n",
      "|   307|                 0.0| 23184|        0|\n",
      "|   404|                 0.0|   252|        0|\n",
      "|   200|2.041473315588219E-4|254718|       52|\n",
      "+------+--------------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT status, mean(Churn) as Churn,count(*) as cnt,sum(Churn) as sum_churn\n",
    "          FROM df_loss_table\n",
    "          GROUP BY status\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程\n",
    "熟悉了数据之后，就可以构建你认为会对训练模型帮助最大的特征。要处理完整数据集，你可以按照下述步骤：\n",
    "- 写一个脚本来从小数据集中提取你需要的特征\n",
    "- 确保你的脚本可以拓展到大数据集上，使用之前教过的最佳实践原则\n",
    "- 在完整数据集上运行你的脚本，按运行情况调试代码\n",
    "\n",
    "如果是在教室的 workspace，你可以直接用里面提供的小数据集来提取特征。确保当你开始使用 Spark 集群的时候，把上述的成果迁移到大数据集上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = df_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', Churn=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "windowval = Window.partitionBy(\"userId\").orderBy((\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "user_log_valid = user_log.withColumn(\"phase\", Fsum(\"Churn\").over(windowval))\n",
    "user_log_valid = user_log_valid.filter((user_log_valid.phase==0) | (user_log_valid.page=='Cancellation Confirmation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_var1 = user_log_valid.groupBy('userID').agg({'Churn':'max','song':'count'}).withColumnRenamed(\"max(Churn)\", \"Churn\").withColumnRenamed('count(song)','song_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_var2 = user_log_valid.filter(user_log_valid.level=='paid').groupBy('userID').agg({'level':'count'}).withColumnRenamed('count(level)','paid_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_var3 = user_log_valid.filter(user_log_valid.level=='free').groupBy('userID').agg({'level':'count'}).withColumnRenamed('count(level)','free_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = (df_user_var1.join(df_user_var2,['userID'],\"left\")).join(df_user_var3, ['userID'],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = df_user.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = df_user.drop('userID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(song_cnt=275, Churn=0, paid_cnt=0, free_cnt=381)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模\n",
    "将完整数据集分成训练集、测试集和验证集。测试几种你学过的机器学习方法。评价不同机器学习方法的准确率，根据情况调节参数。根据准确率你挑选出表现最好的那个模型，然后报告在训练集上的结果。因为流失顾客数据集很小，我建议选用 F1 score 作为优化指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_user.randomSplit([0.6, 0.4], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(song_cnt=8, Churn=1, paid_cnt=0, free_cnt=10),\n",
       " Row(song_cnt=150, Churn=0, paid_cnt=0, free_cnt=201),\n",
       " Row(song_cnt=4079, Churn=0, paid_cnt=4825, free_cnt=0),\n",
       " Row(song_cnt=2841, Churn=1, paid_cnt=2859, free_cnt=578),\n",
       " Row(song_cnt=820, Churn=0, paid_cnt=858, free_cnt=144)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"song_cnt\",'paid_cnt','free_cnt'], outputCol=\"features\")\n",
    "indexer = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
    "\n",
    "lr =  LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)\n",
    "\n",
    "pipeline = Pipeline(stages=[ assembler, indexer, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "print(results.filter(results.label == results.prediction).count())\n",
    "print(results.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam,[0.0, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_q1 = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "results_cv = cvModel_q1.transform(test)\n",
    "print(results_cv.filter(results_cv.label == results_cv.prediction).count())\n",
    "print(results_cv.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最后一步\n",
    "清理你的代码，添加注释和重命名变量，使得代码更易读和易于维护。参考 Spark 项目概述页面和数据科学家毕业项目审阅要求，确保你的项目包含了毕业项目要求的所有内容，并且满足所有审阅要求。记得在 GitHub 代码库里包含一份全面的文档——README文件，以及一个网络应用程序或博客文章。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
